{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries we'll need \n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import datasets, neighbors, linear_model\n",
    "from sklearn.svm import SVC\n",
    "import scipy.stats\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import csv\n",
    "from PorterStemmer import PorterStemmer\n",
    "import random\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in data - train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fixed_combined_train_files_plscommas.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('fixed_combined_dev_files.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting appropriate columns - train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df[['sentence','gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = df_dev[['sentence','gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((869536, 2), (125950, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing - Removing punctuation, Porter Stemming, and lowercasing \n",
    "\n",
    "Note: Must recount unigram and bigram frequencies for each type of preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        #text = str(text).encode('utf-8').replace(punctuation, ' ')\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    #text = re.sub( '\\s+', ' ', text ).encode('utf-8').strip()\n",
    "    text = re.sub( '\\s+', ' ', text ).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stem(text):\n",
    "    p = PorterStemmer()\n",
    "    return p.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below: exclusively for choosing a preprocessing technique\n",
    "\n",
    "5/28/2018 1:35pm REMOVE PUNCTUATION AS PREPROCESSING\n",
    "\n",
    "06/03/2016 REMOVE PUNCTUATION, LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "data_train.loc[:, 'sentence'] = data_train.loc[:, 'sentence'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "data_dev.loc[:, 'sentence'] = data_dev.loc[:, 'sentence'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "data_train.loc[:, 'sentence'] = data_train.loc[:, 'sentence'].apply(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "data_dev.loc[:, 'sentence'] = data_dev.loc[:, 'sentence'].apply(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for collecting unigram and bigram frequencies - must call every time a new preprocessing technique is applied "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_unigrams():\n",
    "    all_unigrams = {}\n",
    "    #for every row in the dataframe, get the text of the sentence column, tokenize it, and count every token in it \n",
    "    for index, row in data_train.iterrows():\n",
    "        text = row['sentence']\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            all_unigrams[word] = all_unigrams.get(word,0) + 1\n",
    "    sorted_unigrams = sorted(all_unigrams.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_bigrams():\n",
    "    all_bigrams = {}\n",
    "    for index, row in data_train.iterrows():\n",
    "        text = row['sentence']\n",
    "        words = ['<S>'] + word_tokenize(text) + ['</S>']\n",
    "        for i in range(1, len(words)):\n",
    "            bi = words[i-1] + \" \" + words[i]\n",
    "            all_bigrams[bi] = all_bigrams.get(bi,0) + 1\n",
    "    sorted_bigrams = sorted(all_bigrams.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually collecting frequencies for model being run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_frequencies = sort_unigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_frequencies = sort_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115933"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3146374"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the top 5k unigrams for use as features in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_unigrams = {uni[0]: uni[1] for uni in unigram_frequencies[:5000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bigrams = {bi[0]: bi[1] for bi in bigram_frequencies[:10000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featurizing functions: unigrams, bigrams, unigrams + bigrams, all the rest applied to POS and lemma, LIWC lists, HBR lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given sentence, and list of top unigrams and bigrams\n",
    "# returns feature dicts for unigrams and bigrams\n",
    "def unigrams(text):\n",
    "    words = word_tokenize(text)\n",
    "    final = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word in top_unigrams:\n",
    "            final[word] = final.get(word,0) + 1\n",
    "        else:\n",
    "            final['UNK'] = final.get('UNK',0) + 1\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_unigrams(text):\n",
    "    words = word_tokenize(text)\n",
    "    final = {}\n",
    "    \n",
    "    for word in words:\n",
    "        final[word] = final.get(word,0) + 1\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_bigrams(text):\n",
    "    words = ['<S>'] + word_tokenize(text) + ['</S>']\n",
    "    final = {}\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        bi = words[i-1] + \" \" + words[i]\n",
    "        final[bi] = final.get(bi,0) + 1\n",
    "    return final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(text):\n",
    "    words = ['<S>'] + word_tokenize(text) + ['</S>']\n",
    "    final = {}\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        bi = words[i-1] + \" \" + words[i]\n",
    "        if bi in top_bigrams:\n",
    "            final[bi] = final.get(bi,0) + 1\n",
    "        else:\n",
    "            final['UNK UNK'] = final.get('UNK UNK',0) + 1\n",
    "    return final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_unigrams(text):\n",
    "    words = word_tokenize(text)\n",
    "    tags = [pair[1] for pair in pos_tag(words)]\n",
    "    final = {}\n",
    "    for tag in tags:\n",
    "        final[tag] = final.get(tag,0)+1\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually featurizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dicts_train = []\n",
    "labels_train = []\n",
    "for index, row in data_train.iterrows():\n",
    "    text = row['sentence']\n",
    "    label = row['gender']\n",
    "    feature_dict = {**all_unigrams(text),**all_bigrams(text)}\n",
    "    feat_dicts_train.append(feature_dict)\n",
    "    labels_train.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dicts_dev = []\n",
    "labels_dev = []\n",
    "for index, row in data_dev.iterrows():\n",
    "    text = row['sentence']\n",
    "    label = row['gender']\n",
    "    feature_dict = {**all_unigrams(text),**all_bigrams(text)}\n",
    "    feat_dicts_dev.append(feature_dict)\n",
    "    labels_dev.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting dictionary of dictionaries to matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer(sparse=True)\n",
    "features_train = vectorizer.fit_transform(feat_dicts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dev = vectorizer.transform(feat_dicts_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((869536, 3262307), 869536, (125950, 3262307), 125950)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape, len(labels_train), features_dev.shape, len(labels_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: re-weighted features_train and features_dev with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_tfidf = tfidf_transformer.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dev_tfidf = tfidf_transformer.transform(features_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((869536, 3262307), (125950, 3262307))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_tfidf.shape,features_dev_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 20 epochs took 53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   54.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   54.1s finished\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(verbose=3,solver='sag',class_weight = {\"male\":0.25, \"female\":0.75})\n",
    "log_model = logistic.fit(features_train_tfidf,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: SGD with hinge loss and batch generator (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_minibatches2(chunksize, datasize):\n",
    "    chunkstartmarker = 0\n",
    "    while chunkstartmarker < datasize:\n",
    "        if chunkstartmarker + chunksize < datasize:\n",
    "            X_chunk = features_train_tfidf[chunkstartmarker:chunkstartmarker+chunksize]\n",
    "            y_chunk = labels_train_tf[chunkstartmarker:chunkstartmarker+chunksize]\n",
    "            chunkstartmarker+=chunksize\n",
    "        else:\n",
    "            X_chunk = features_train[chunkstartmarker:datasize]\n",
    "            y_chunk = labels_train[chunkstartmarker]\n",
    "            chunkstartmarker = datasize\n",
    "        yield X_chunk,y_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_minibatches(chunksize, datasize):\n",
    "    chunkstartmarker = 0\n",
    "    while chunkstartmarker < datasize:\n",
    "        X_chunk = features_train_tfidf[chunkstartmarker:chunkstartmarker+chunksize]\n",
    "        y_chunk = labels_train[chunkstartmarker:chunkstartmarker+chunksize]\n",
    "        chunkstartmarker+=chunksize\n",
    "        yield X_chunk,y_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcherator = iter_minibatches(chunksize=108692,datasize=869536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='hinge',verbose=3,class_weight = {\"male\":0.25, \"female\":0.75},max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 20.40, NNZs: 3215895, Bias: -0.271719, T: 869536, Avg. loss: 0.892494\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.41, NNZs: 3220752, Bias: -0.279386, T: 1739072, Avg. loss: 0.890188\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 20.43, NNZs: 3221461, Bias: -0.279286, T: 2608608, Avg. loss: 0.890033\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 20.44, NNZs: 3221664, Bias: -0.276347, T: 3478144, Avg. loss: 0.891505\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 20.43, NNZs: 3221723, Bias: -0.281534, T: 4347680, Avg. loss: 0.888259\n",
      "Total training time: 3.70 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 20.42, NNZs: 3221758, Bias: -0.284463, T: 5217216, Avg. loss: 0.887753\n",
      "Total training time: 4.51 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 20.43, NNZs: 3221871, Bias: -0.280602, T: 6086752, Avg. loss: 0.891570\n",
      "Total training time: 5.24 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 20.43, NNZs: 3221886, Bias: -0.280328, T: 6956288, Avg. loss: 0.890111\n",
      "Total training time: 5.95 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 20.43, NNZs: 3221889, Bias: -0.280362, T: 7825824, Avg. loss: 0.889714\n",
      "Total training time: 6.67 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 20.43, NNZs: 3221889, Bias: -0.281314, T: 8695360, Avg. loss: 0.888909\n",
      "Total training time: 7.39 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 20.43, NNZs: 3221890, Bias: -0.282075, T: 9564896, Avg. loss: 0.888776\n",
      "Total training time: 8.12 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 20.43, NNZs: 3221891, Bias: -0.282162, T: 10434432, Avg. loss: 0.889250\n",
      "Total training time: 8.85 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 20.43, NNZs: 3221899, Bias: -0.280915, T: 11303968, Avg. loss: 0.890608\n",
      "Total training time: 9.57 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 20.43, NNZs: 3221899, Bias: -0.281016, T: 12173504, Avg. loss: 0.889537\n",
      "Total training time: 10.31 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281129, T: 13043040, Avg. loss: 0.889433\n",
      "Total training time: 11.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.280677, T: 13912576, Avg. loss: 0.889951\n",
      "Total training time: 12.05 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281669, T: 14782112, Avg. loss: 0.888677\n",
      "Total training time: 13.02 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281319, T: 15651648, Avg. loss: 0.889832\n",
      "Total training time: 13.79 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281394, T: 16521184, Avg. loss: 0.889389\n",
      "Total training time: 14.68 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281583, T: 17390720, Avg. loss: 0.889252\n",
      "Total training time: 15.83 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281415, T: 18260256, Avg. loss: 0.889705\n",
      "Total training time: 16.85 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281910, T: 19129792, Avg. loss: 0.888807\n",
      "Total training time: 17.90 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281916, T: 19999328, Avg. loss: 0.889416\n",
      "Total training time: 18.84 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282147, T: 20868864, Avg. loss: 0.889094\n",
      "Total training time: 19.99 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282026, T: 21738400, Avg. loss: 0.889503\n",
      "Total training time: 20.79 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281645, T: 22607936, Avg. loss: 0.889856\n",
      "Total training time: 21.58 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281855, T: 23477472, Avg. loss: 0.889278\n",
      "Total training time: 22.34 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282148, T: 24347008, Avg. loss: 0.889080\n",
      "Total training time: 23.13 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282148, T: 25216544, Avg. loss: 0.889391\n",
      "Total training time: 23.89 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282006, T: 26086080, Avg. loss: 0.889475\n",
      "Total training time: 24.64 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281974, T: 26955616, Avg. loss: 0.889489\n",
      "Total training time: 25.36 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282050, T: 27825152, Avg. loss: 0.889288\n",
      "Total training time: 26.57 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281834, T: 28694688, Avg. loss: 0.889674\n",
      "Total training time: 27.54 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281599, T: 29564224, Avg. loss: 0.889833\n",
      "Total training time: 28.45 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281803, T: 30433760, Avg. loss: 0.889271\n",
      "Total training time: 29.19 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281745, T: 31303296, Avg. loss: 0.889532\n",
      "Total training time: 29.95 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281803, T: 32172832, Avg. loss: 0.889439\n",
      "Total training time: 30.80 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281803, T: 33042368, Avg. loss: 0.889449\n",
      "Total training time: 31.73 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282056, T: 33911904, Avg. loss: 0.889116\n",
      "Total training time: 32.57 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281951, T: 34781440, Avg. loss: 0.889521\n",
      "Total training time: 33.76 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281884, T: 35650976, Avg. loss: 0.889503\n",
      "Total training time: 34.76 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281813, T: 36520512, Avg. loss: 0.889613\n",
      "Total training time: 35.62 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282046, T: 37390048, Avg. loss: 0.889093\n",
      "Total training time: 36.54 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282028, T: 38259584, Avg. loss: 0.889448\n",
      "Total training time: 37.86 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282050, T: 39129120, Avg. loss: 0.889428\n",
      "Total training time: 39.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282191, T: 39998656, Avg. loss: 0.889248\n",
      "Total training time: 40.31 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282123, T: 40868192, Avg. loss: 0.889538\n",
      "Total training time: 41.62 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282199, T: 41737728, Avg. loss: 0.889287\n",
      "Total training time: 42.96 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281933, T: 42607264, Avg. loss: 0.889820\n",
      "Total training time: 44.29 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282043, T: 43476800, Avg. loss: 0.889320\n",
      "Total training time: 45.11 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282117, T: 44346336, Avg. loss: 0.889343\n",
      "Total training time: 45.88 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282097, T: 45215872, Avg. loss: 0.889440\n",
      "Total training time: 46.61 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282088, T: 46085408, Avg. loss: 0.889419\n",
      "Total training time: 47.33 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282107, T: 46954944, Avg. loss: 0.889374\n",
      "Total training time: 48.08 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282007, T: 47824480, Avg. loss: 0.889545\n",
      "Total training time: 48.81 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282029, T: 48694016, Avg. loss: 0.889413\n",
      "Total training time: 49.55 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282160, T: 49563552, Avg. loss: 0.889238\n",
      "Total training time: 50.28 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282059, T: 50433088, Avg. loss: 0.889576\n",
      "Total training time: 50.99 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282088, T: 51302624, Avg. loss: 0.889420\n",
      "Total training time: 51.71 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281999, T: 52172160, Avg. loss: 0.889550\n",
      "Total training time: 52.44 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282103, T: 53041696, Avg. loss: 0.889340\n",
      "Total training time: 53.14 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282121, T: 53911232, Avg. loss: 0.889328\n",
      "Total training time: 53.86 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282077, T: 54780768, Avg. loss: 0.889518\n",
      "Total training time: 54.60 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282049, T: 55650304, Avg. loss: 0.889547\n",
      "Total training time: 55.32 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282087, T: 56519840, Avg. loss: 0.889365\n",
      "Total training time: 56.05 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.281937, T: 57389376, Avg. loss: 0.889673\n",
      "Total training time: 56.77 seconds.\n",
      "-- Epoch 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282053, T: 58258912, Avg. loss: 0.889321\n",
      "Total training time: 57.50 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282042, T: 59128448, Avg. loss: 0.889490\n",
      "Total training time: 58.21 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282147, T: 59997984, Avg. loss: 0.889254\n",
      "Total training time: 58.92 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282080, T: 60867520, Avg. loss: 0.889504\n",
      "Total training time: 59.66 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282026, T: 61737056, Avg. loss: 0.889498\n",
      "Total training time: 60.37 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282160, T: 62606592, Avg. loss: 0.889225\n",
      "Total training time: 61.10 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282174, T: 63476128, Avg. loss: 0.889426\n",
      "Total training time: 61.81 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282099, T: 64345664, Avg. loss: 0.889470\n",
      "Total training time: 62.53 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282197, T: 65215200, Avg. loss: 0.889286\n",
      "Total training time: 63.23 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282091, T: 66084736, Avg. loss: 0.889565\n",
      "Total training time: 64.01 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282143, T: 66954272, Avg. loss: 0.889344\n",
      "Total training time: 64.74 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282100, T: 67823808, Avg. loss: 0.889504\n",
      "Total training time: 65.47 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282206, T: 68693344, Avg. loss: 0.889287\n",
      "Total training time: 66.33 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282124, T: 69562880, Avg. loss: 0.889531\n",
      "Total training time: 67.04 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282153, T: 70432416, Avg. loss: 0.889396\n",
      "Total training time: 67.77 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282290, T: 71301952, Avg. loss: 0.889233\n",
      "Total training time: 68.51 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282191, T: 72171488, Avg. loss: 0.889533\n",
      "Total training time: 69.46 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282200, T: 73041024, Avg. loss: 0.889407\n",
      "Total training time: 70.56 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282243, T: 73910560, Avg. loss: 0.889327\n",
      "Total training time: 71.40 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282293, T: 74780096, Avg. loss: 0.889360\n",
      "Total training time: 72.16 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282143, T: 75649632, Avg. loss: 0.889681\n",
      "Total training time: 72.97 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282121, T: 76519168, Avg. loss: 0.889506\n",
      "Total training time: 73.99 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282090, T: 77388704, Avg. loss: 0.889519\n",
      "Total training time: 74.79 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282106, T: 78258240, Avg. loss: 0.889415\n",
      "Total training time: 75.84 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282226, T: 79127776, Avg. loss: 0.889249\n",
      "Total training time: 76.63 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282174, T: 79997312, Avg. loss: 0.889492\n",
      "Total training time: 77.33 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282174, T: 80866848, Avg. loss: 0.889436\n",
      "Total training time: 78.69 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282230, T: 81736384, Avg. loss: 0.889330\n",
      "Total training time: 79.54 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282250, T: 82605920, Avg. loss: 0.889368\n",
      "Total training time: 80.34 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282222, T: 83475456, Avg. loss: 0.889425\n",
      "Total training time: 81.07 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282198, T: 84344992, Avg. loss: 0.889458\n",
      "Total training time: 81.80 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282194, T: 85214528, Avg. loss: 0.889446\n",
      "Total training time: 82.69 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282193, T: 86084064, Avg. loss: 0.889397\n",
      "Total training time: 83.56 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 20.43, NNZs: 3221901, Bias: -0.282244, T: 86953600, Avg. loss: 0.889336\n",
      "Total training time: 84.58 seconds.\n"
     ]
    }
   ],
   "source": [
    "sgd_model = sgd.fit(features_train_tfidf,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 21.45, NNZs: 2546972, Bias: -0.174077, T: 108692, Avg. loss: 0.993869\n",
      "Total training time: 0.12 seconds.\n",
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n",
      "Norm: 21.03, NNZs: 2678254, Bias: -0.203962, T: 108692, Avg. loss: 1.004631\n",
      "Total training time: 0.13 seconds.\n",
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n",
      "Norm: 20.80, NNZs: 2792711, Bias: -0.150260, T: 108692, Avg. loss: 0.964628\n",
      "Total training time: 0.23 seconds.\n",
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n",
      "Norm: 20.73, NNZs: 2899863, Bias: -0.066163, T: 108692, Avg. loss: 0.883742\n",
      "Total training time: 0.11 seconds.\n",
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n",
      "Norm: 20.69, NNZs: 2999927, Bias: -0.076528, T: 108692, Avg. loss: 0.839622\n",
      "Total training time: 0.23 seconds.\n",
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n",
      "Norm: 20.70, NNZs: 3087096, Bias: -0.063502, T: 108692, Avg. loss: 0.820627\n",
      "Total training time: 0.11 seconds.\n",
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n",
      "Norm: 20.76, NNZs: 3170707, Bias: -0.121287, T: 108692, Avg. loss: 0.838387\n",
      "Total training time: 0.22 seconds.\n",
      "(108692, 3262307) 108692\n",
      "-- Epoch 1\n",
      "Norm: 20.85, NNZs: 3253127, Bias: -0.132456, T: 108692, Avg. loss: 0.849331\n",
      "Total training time: 0.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "batcherator = iter_minibatches(chunksize=108692,datasize=869536)\n",
    "for X_chunk, y_chunk in batcherator:\n",
    "    print(X_chunk.shape,len(y_chunk))\n",
    "    sgd_model = sgd.partial_fit(X_chunk, y_chunk,classes=np.unique(labels_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_predictions = logistic.predict(features_dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_predictions = sgd_model.predict(features_dev_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression result: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     female       0.45      0.65      0.53     31498\n",
      "       male       0.86      0.74      0.80     94452\n",
      "\n",
      "avg / total       0.76      0.72      0.73    125950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression result: \")\n",
    "print(classification_report(labels_dev, logistic_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM result: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     female       0.34      0.70      0.46     31498\n",
      "       male       0.85      0.54      0.66     94452\n",
      "\n",
      "avg / total       0.72      0.58      0.61    125950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM result: \")\n",
    "print(classification_report(labels_dev, sgd_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out the most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(log_model.coef_.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,weight in zip(names,weights):\n",
    "    weight_dict[name] = weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_weights = sorted(weight_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', -15.28234998805659),\n",
       " ('ms', -14.410394364231394),\n",
       " ('it husband', -14.0561093838102),\n",
       " ('husband', -11.449207288063292),\n",
       " ('girl', -10.013808216583808),\n",
       " ('mother', -8.729180563359714),\n",
       " ('mrs', -8.099146146971655),\n",
       " ('actress', -8.0961787054303),\n",
       " ('hillary', -7.903183282896676),\n",
       " ('daughter', -7.8972067366029215),\n",
       " ('lady', -7.8659130067956085),\n",
       " ('female', -7.407729584877959),\n",
       " ('devos', -6.997176335836824),\n",
       " ('women', -6.434847439771052),\n",
       " ('mrs names', -6.368574044697234),\n",
       " ('mom', -6.102296303229272),\n",
       " ('server', -5.979593459020139),\n",
       " ('beyoncé', -5.80772439226515),\n",
       " ('clinton', -5.784430928882867),\n",
       " ('<S> ms', -5.643238260721736),\n",
       " ('pregnant', -5.490622542304049),\n",
       " ('girls', -5.3118364176968385),\n",
       " ('sister', -5.184267153166717),\n",
       " ('feminist', -5.123609512959551),\n",
       " ('pregnancy', -4.973782757806087),\n",
       " ('boyfriend', -4.967090357230357),\n",
       " ('queen', -4.900113576404142),\n",
       " ('emails', -4.79800945117063),\n",
       " ('lock it', -4.700528759089891),\n",
       " ('as secretary', -4.616822771514494),\n",
       " ('email', -4.590227690501396),\n",
       " ('dress', -4.520326103972511),\n",
       " ('it father', -4.46915827429288),\n",
       " ('first lady', -4.3664794470973),\n",
       " ('it boyfriend', -4.327839352489271),\n",
       " ('baby', -4.301232024891552),\n",
       " ('gender', -4.216425015349216),\n",
       " ('ms names', -4.2110030557306),\n",
       " ('classified', -4.158438359579325),\n",
       " ('le pen', -4.037011233278365),\n",
       " ('state department', -4.033985316847657),\n",
       " ('democratic presidential', -4.023398210791345),\n",
       " ('trump </S>', -4.000279063330549),\n",
       " ('of state', -3.861047022286088),\n",
       " ('raped', -3.8480712388165403),\n",
       " ('fashion', -3.8455586981317755),\n",
       " ('it emails', -3.801985042459904),\n",
       " ('it son', -3.767761930878008),\n",
       " ('women s', -3.7451806867097175),\n",
       " ('madonna', -3.724481825697224)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_weights[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('breitbart', 3.029555373229729),\n",
       " ('it inauguration', 3.034438422704227),\n",
       " ('warriors', 3.0545243040283894),\n",
       " ('players', 3.1148744381856477),\n",
       " ('indiana', 3.1283241604414855),\n",
       " ('chairman', 3.1468328759835833),\n",
       " ('scalia', 3.1549902213016616),\n",
       " ('priebus', 3.1592727298344805),\n",
       " ('league', 3.163718699604721),\n",
       " ('<S> president', 3.164317479983277),\n",
       " ('duterte', 3.164744116316276),\n",
       " ('nato', 3.178603467516678),\n",
       " ('christie', 3.195267414715662),\n",
       " ('comey', 3.2028962576833746),\n",
       " ('father', 3.231586623583917),\n",
       " ('wife', 3.234055969562576),\n",
       " ('facebook </S>', 3.238604308041458),\n",
       " ('twitter bobpricebbtx', 3.244548638584041),\n",
       " ('knicks', 3.2893605096940175),\n",
       " ('football', 3.3263408650220674),\n",
       " ('congressman', 3.327704910980327),\n",
       " ('baseball', 3.4093706037143767),\n",
       " ('nfl', 3.551173167960581),\n",
       " ('ryan', 3.5585256705591646),\n",
       " ('milo', 3.5712744501348963),\n",
       " ('yankees', 3.57216836471881),\n",
       " ('businessman', 3.6482757380133988),\n",
       " ('it girlfriend', 3.6512450488367034),\n",
       " ('boy', 3.7157066011985482),\n",
       " ('it presidency', 3.7417963512074106),\n",
       " ('romney', 3.7474017652238714),\n",
       " ('musk', 3.797158440762813),\n",
       " ('<S> trump', 3.827077770622292),\n",
       " ('sessions', 3.8369030803657664),\n",
       " ('cruz', 3.9041907854993934),\n",
       " ('ali', 4.1104774908601165),\n",
       " ('my husband', 4.236261926726321),\n",
       " ('quarterback', 4.260148062491549),\n",
       " ('prince', 4.325214110165995),\n",
       " ('mets', 4.332795481273852),\n",
       " ('president', 4.387296753964309),\n",
       " ('pence', 4.534254188485571),\n",
       " ('<S> mr', 4.839283174897367),\n",
       " ('rubio', 4.966607828250133),\n",
       " ('it administration', 5.036741675907579),\n",
       " ('guy', 5.636450465061789),\n",
       " ('man', 6.308674384770179),\n",
       " ('trump', 8.17882638593814),\n",
       " ('mr', 9.120183957153026),\n",
       " ('it wife', 12.258148800348879)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_weights[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
